{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHxXgknOFrrOFe5XcnybYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taishi-N324/CPP/blob/main/rnn_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU使用に指定**"
      ],
      "metadata": {
        "id": "IW1tX3dwf2sp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGKao-5ubmtl",
        "outputId": "71315e66-76df-4f07-9e9a-15f1bcb9f8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'light_enja2'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 56 (delta 22), reused 33 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/Taishi-N324/light_enja2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "import datetime, os\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import files\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "vbKuFHCCdIvv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps92UvfjcTf_",
        "outputId": "c9a7fafc-5869-4296-b215-4780bb6e20cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_dir = \"light_enja2/corpus/\"\n",
        "\n",
        "with open(file_dir + \"train.en\") as f:\n",
        "  lines_en = f.read().split(\"\\n\")[:-1]\n",
        "with open(file_dir + \"train.ja\") as f:\n",
        "  lines_ja = f.read().split(\"\\n\")[:-1]"
      ],
      "metadata": {
        "id": "1s_-I9GXcO6z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for i in range(100000):\n",
        "  en = lines_en[i]\n",
        "  ja = \"[start] \" + lines_ja[i] + \" [end]\"\n",
        "\n",
        "  text_pairs.append((en,ja))"
      ],
      "metadata": {
        "id": "f7ks0q7ncPJm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "Mi62h2b6c52o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 30\n",
        "batch_size = 64\n",
        "embed_dim = 256\n",
        "latent_dim = 1024"
      ],
      "metadata": {
        "id": "2gaPdyDxd1VZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation \n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_japanese_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_japanese_texts)"
      ],
      "metadata": {
        "id": "f31caChjdd-3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, ja):\n",
        "    eng = source_vectorization(eng)\n",
        "    ja = target_vectorization(ja)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"japanese\": ja[:, :-1],\n",
        "    }, ja[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ja_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ja_texts = list(ja_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ja_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "iuTSf6aOdyi0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['japanese'].shape: {inputs['japanese'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu29tbCoejRu",
        "outputId": "c6f8c682-49f5-40d0-c3d4-792d9b9f54a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 30)\n",
            "inputs['japanese'].shape: (64, 30)\n",
            "targets.shape: (64, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "0N6pbWEYevho"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"japanese\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ],
      "metadata": {
        "id": "N3c5V1EUe33N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "log = seq2seq_rnn.fit(train_ds, \n",
        "                      epochs=25, \n",
        "                      validation_data=val_ds, \n",
        "                      callbacks=[tensorboard_callback])\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Mitnzse8r8",
        "outputId": "a32d7484-6aea-4804-bf9f-d35721410e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1094/1094 [==============================] - 155s 128ms/step - loss: 1.6191 - accuracy: 0.2819 - val_loss: 1.4666 - val_accuracy: 0.3206\n",
            "Epoch 2/25\n",
            " 192/1094 [====>.........................] - ETA: 1:34 - loss: 1.5196 - accuracy: 0.3146"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ja_vocab = target_vectorization.get_vocabulary()\n",
        "ja_index_lookup = dict(zip(range(len(ja_vocab)), ja_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = ja_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "2S0tjfF4fDmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}