{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXGArUp9yXLEpTNOZ6muAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taishi-N324/CPP/blob/main/layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "必要に応じて !pip install \n",
        "'''\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import datetime, os\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import torch\n",
        "import sys\n",
        "from torch import nn\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn import LayerNorm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "BongSnDUxdjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8f416b-6fd0-41ca-e9f0-ba58697239f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ここの部分は後々改良\n",
        "'''\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Bo1SCV-LyIgs",
        "outputId": "0af24cce-6900-4136-df46-4ae73af83c41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a89539d3-f418-4388-a9e9-1c5a932c929a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a89539d3-f418-4388-a9e9-1c5a932c929a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ja-en.csv to ja-en.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#データ数増やす場合はgoogle driveに\n",
        "#csvかtxt..etc..\n",
        "\n",
        "\n",
        "name_csv = \"ja-en.csv\"\n",
        "\n",
        "ja_train_text = []\n",
        "en_train_text = []\n",
        "train_pairs = []\n",
        "val_pairs = []\n",
        "test_pairs = []\n",
        "\n",
        "with open(name_csv, newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  \n",
        "  for row in reader:\n",
        "    ja_train_text.append(row['Japanese'])\n",
        "    en_train_text.append(row['English'])\n",
        "    row['English'] = \"\\\\\" + \" \" + row['English'] +\"|||\"\n",
        "    #ここの感覚は調整する\n",
        "    #trainとval　を同じデータ、シャッフルするだけにしたらどうなるのか\n",
        "\n",
        "    #学習データ：検証データ：テストデータ=8,1,1がよいらしいので。\n",
        "    train_pairs.append((row['Japanese'],row['English']))\n",
        "    random.shuffle(train_pairs)\n",
        "  #print(len(train_pairs))\n",
        "\n",
        "  for i in range(len(train_pairs)//10):\n",
        "    val_pairs.append(train_pairs.pop(0))\n",
        "    test_pairs.append(train_pairs.pop(0))\n",
        "  #print(len(train_pairs))"
      ],
      "metadata": {
        "id": "7mAfSAXFyNNn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p9tYrLL7vLxc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, pad_idx: int = 0) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding_layer = nn.Embedding(\n",
        "            num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=pad_idx\n",
        "        )\n",
        "\n",
        "    def forward(self, input_batch: torch.Tensor) -> torch.Tensor:\n",
        "        return self.embedding_layer(input_batch)\n",
        "    \n",
        "    \n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int) -> None:\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear2(relu(self.linear1(x)))\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class AddPositionalEncoding(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model: int, max_len: int, device: torch.device = torch.device(\"cpu\")\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        positional_encoding_weight: torch.Tensor = self._initialize_weight().to(device)\n",
        "        self.register_buffer(\"positional_encoding_weight\", positional_encoding_weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.positional_encoding_weight[:seq_len, :].unsqueeze(0)\n",
        "\n",
        "    def _get_positional_encoding(self, pos: int, i: int) -> float:\n",
        "        w = pos / (10000 ** (((2 * i) // 2) / self.d_model))\n",
        "        if i % 2 == 0:\n",
        "            return np.sin(w)\n",
        "        else:\n",
        "            return np.cos(w)\n",
        "\n",
        "    def _initialize_weight(self) -> torch.Tensor:\n",
        "        positional_encoding_weight = [\n",
        "            [self._get_positional_encoding(pos, i) for i in range(1, self.d_model + 1)]\n",
        "            for pos in range(1, self.max_len + 1)\n",
        "        ]\n",
        "        return torch.tensor(positional_encoding_weight).float()\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.d_k = d_model // h\n",
        "        self.d_v = d_model // h\n",
        "\n",
        "        #\n",
        "        self.W_k = nn.Parameter(\n",
        "            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)\n",
        "        )\n",
        "\n",
        "        self.W_q = nn.Parameter(\n",
        "            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)\n",
        "        )\n",
        "\n",
        "        self.W_v = nn.Parameter(\n",
        "            torch.Tensor(h, d_model, self.d_v)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)\n",
        "        )\n",
        "\n",
        "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.d_k)\n",
        "\n",
        "        self.linear = nn.Linear(h * self.d_v, d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        mask_3d: torch.Tensor = None,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        batch_size, seq_len = q.size(0), q.size(1)\n",
        "\n",
        "        \"\"\"repeat Query,Key,Value by num of heads\"\"\"\n",
        "        q = q.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model\n",
        "        k = k.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model\n",
        "        v = v.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model\n",
        "\n",
        "        \"\"\"Linear before scaled dot product attention\"\"\"\n",
        "        q = torch.einsum(\n",
        "            \"hijk,hkl->hijl\", (q, self.W_q)\n",
        "        )  # head, batch_size, d_k, seq_len\n",
        "        k = torch.einsum(\n",
        "            \"hijk,hkl->hijl\", (k, self.W_k)\n",
        "        )  # head, batch_size, d_k, seq_len\n",
        "        v = torch.einsum(\n",
        "            \"hijk,hkl->hijl\", (v, self.W_v)\n",
        "        )  # head, batch_size, d_k, seq_len\n",
        "\n",
        "        \"\"\"Split heads\"\"\"\n",
        "        q = q.view(self.h * batch_size, seq_len, self.d_k)\n",
        "        k = k.view(self.h * batch_size, seq_len, self.d_k)\n",
        "        v = v.view(self.h * batch_size, seq_len, self.d_v)\n",
        "\n",
        "        if mask_3d is not None:\n",
        "            mask_3d = mask_3d.repeat(self.h, 1, 1)\n",
        "\n",
        "        \"\"\"Scaled dot product attention\"\"\"\n",
        "        attention_output = self.scaled_dot_product_attention(\n",
        "            q, k, v, mask_3d\n",
        "        )  # (head*batch_size, seq_len, d_model)\n",
        "\n",
        "        attention_output = torch.chunk(attention_output, self.h, dim=0)\n",
        "        attention_output = torch.cat(attention_output, dim=2)\n",
        "\n",
        "        \"\"\"Linear after scaled dot product attention\"\"\"\n",
        "        output = self.linear(attention_output)\n",
        "        return output\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,  # target\n",
        "        k: torch.Tensor,  # source\n",
        "        v: torch.Tensor,  # source\n",
        "        mask: torch.Tensor = None,\n",
        "    ) -> torch.Tensor:\n",
        "        scalar = np.sqrt(self.d_k)\n",
        "        attention_weight = torch.matmul(q, torch.transpose(k, 1, 2)) / scalar\n",
        "        if mask is not None:\n",
        "            if mask.dim() != attention_weight.dim():\n",
        "                raise ValueError(\n",
        "                    \"mask.dim != attention_weight.dim, mask.dim={}, attention_weight.dim={}\".format(\n",
        "                        mask.dim(), attention_weight.dim()\n",
        "                    )\n",
        "                )\n",
        "            attention_weight = attention_weight.data.masked_fill_(\n",
        "                mask, -torch.finfo(torch.float).max\n",
        "            )\n",
        "        attention_weight = nn.functional.softmax(attention_weight, dim=2)\n",
        "        return torch.matmul(attention_weight, v)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_ff: int,\n",
        "        heads_num: int,\n",
        "        dropout_rate: float,\n",
        "        layer_norm_eps: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, heads_num)\n",
        "        self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm_self_attention = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "        self.src_tgt_attention = MultiHeadAttention(d_model, heads_num)\n",
        "        self.dropout_src_tgt_attention = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm_src_tgt_attention = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "        self.ffn = FFN(d_model, d_ff)\n",
        "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm_ffn = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,  # Decoder input\n",
        "        src: torch.Tensor,  # Encoder output\n",
        "        mask_src_tgt: torch.Tensor,\n",
        "        mask_self: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        tgt = self.layer_norm_self_attention(\n",
        "            tgt + self.__self_attention_block(tgt, mask_self)\n",
        "        )\n",
        "\n",
        "        x = self.layer_norm_src_tgt_attention(\n",
        "            tgt + self.__src_tgt_attention_block(src, tgt, mask_src_tgt)\n",
        "        )\n",
        "\n",
        "        x = self.layer_norm_ffn(x + self.__feed_forward_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __src_tgt_attention_block(\n",
        "        self, src: torch.Tensor, tgt: torch.Tensor, mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        return self.dropout_src_tgt_attention(\n",
        "            self.src_tgt_attention(tgt, src, src, mask)\n",
        "        )\n",
        "\n",
        "    def __self_attention_block(\n",
        "        self, x: torch.Tensor, mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        return self.dropout_self_attention(self.self_attention(x, x, x, mask))\n",
        "\n",
        "    def __feed_forward_block(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.dropout_ffn(self.ffn(x))\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tgt_vocab_size: int,\n",
        "        max_len: int,\n",
        "        pad_idx: int,\n",
        "        d_model: int,\n",
        "        N: int,\n",
        "        d_ff: int,\n",
        "        heads_num: int,\n",
        "        dropout_rate: float,\n",
        "        layer_norm_eps: float,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(tgt_vocab_size, d_model, pad_idx)\n",
        "        self.positional_encoding = AddPositionalEncoding(d_model, max_len, device)\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderLayer(\n",
        "                    d_model, d_ff, heads_num, dropout_rate, layer_norm_eps\n",
        "                )\n",
        "                for _ in range(N)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,  # Decoder input\n",
        "        src: torch.Tensor,  # Encoder output\n",
        "        mask_src_tgt: torch.Tensor,\n",
        "        mask_self: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            tgt = decoder_layer(\n",
        "                tgt,\n",
        "                src,\n",
        "                mask_src_tgt,\n",
        "                mask_self,\n",
        "            )\n",
        "        return tgt\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_ff: int,\n",
        "        heads_num: int,\n",
        "        dropout_rate: float,\n",
        "        layer_norm_eps: float,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, heads_num)\n",
        "        self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm_self_attention = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "        self.ffn = FFN(d_model, d_ff)\n",
        "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm_ffn = LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        x = self.layer_norm_self_attention(self.__self_attention_block(x, mask) + x)\n",
        "        x = self.layer_norm_ffn(self.__feed_forward_block(x) + x)\n",
        "        return x\n",
        "\n",
        "    def __self_attention_block(\n",
        "        self, x: torch.Tensor, mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        self attention block\n",
        "        \"\"\"\n",
        "        x = self.multi_head_attention(x, x, x, mask)\n",
        "        return self.dropout_self_attention(x)\n",
        "\n",
        "    def __feed_forward_block(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        feed forward block\n",
        "        \"\"\"\n",
        "        return self.dropout_ffn(self.ffn(x))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        max_len: int,\n",
        "        pad_idx: int,\n",
        "        d_model: int,\n",
        "        N: int,\n",
        "        d_ff: int,\n",
        "        heads_num: int,\n",
        "        dropout_rate: float,\n",
        "        layer_norm_eps: float,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, d_model, pad_idx)\n",
        "\n",
        "        self.positional_encoding = AddPositionalEncoding(d_model, max_len, device)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerEncoderLayer(\n",
        "                    d_model, d_ff, heads_num, dropout_rate, layer_norm_eps\n",
        "                )\n",
        "                for _ in range(N)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, mask)\n",
        "        return x    \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk import bleu_score\n",
        "\n",
        "#参照文\n"
      ],
      "metadata": {
        "id": "Z-gb-hgvc1Nr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size: int,\n",
        "        tgt_vocab_size: int,\n",
        "        max_len: int,\n",
        "        d_model: int = 512,\n",
        "        heads_num: int = 8,\n",
        "        d_ff: int = 2048,\n",
        "        N: int = 6,\n",
        "        dropout_rate: float = 0.1,\n",
        "        layer_norm_eps: float = 1e-5,\n",
        "        pad_idx: int = 0,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.heads_num = heads_num\n",
        "        self.d_ff = d_ff\n",
        "        self.N = N\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "\n",
        "        self.encoder = TransformerEncoder(\n",
        "            src_vocab_size,\n",
        "            max_len,\n",
        "            pad_idx,\n",
        "            d_model,\n",
        "            N,\n",
        "            d_ff,\n",
        "            heads_num,\n",
        "            dropout_rate,\n",
        "            layer_norm_eps,\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        self.decoder = TransformerDecoder(\n",
        "            tgt_vocab_size,\n",
        "            max_len,\n",
        "            pad_idx,\n",
        "            d_model,\n",
        "            N,\n",
        "            d_ff,\n",
        "            heads_num,\n",
        "            dropout_rate,\n",
        "            layer_norm_eps,\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        ----------\n",
        "        src : torch.Tensor\n",
        "            単語のid列. [batch_size, max_len]\n",
        "        tgt : torch.Tensor\n",
        "            単語のid列. [batch_size, max_len]\n",
        "        \"\"\"\n",
        "\n",
        "        # mask\n",
        "        pad_mask_src = self._pad_mask(src)\n",
        "\n",
        "        src = self.encoder(src, pad_mask_src)\n",
        "\n",
        "        # if tgt is not None:\n",
        "\n",
        "        # target系列の\"0(BOS)~max_len-1\"(max_len-1系列)までを入力し、\"1~max_len\"(max_len-1系列)を予測する\n",
        "        mask_self_attn = torch.logical_or(\n",
        "            self._subsequent_mask(tgt), self._pad_mask(tgt)\n",
        "        )\n",
        "        dec_output = self.decoder(tgt, src, pad_mask_src, mask_self_attn)\n",
        "\n",
        "        return self.linear(dec_output)\n",
        "\n",
        "    def _pad_mask(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"単語のid列(ex:[[4,1,9,11,0,0,0...],[4,1,9,11,0,0,0...],[4,1,9,11,0,0,0...]...])からmaskを作成する.\n",
        "        Parameters:\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            単語のid列. [batch_size, max_len]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        mask = x.eq(self.pad_idx)  # 0 is <pad> in vocab\n",
        "        mask = mask.unsqueeze(1)\n",
        "        mask = mask.repeat(1, seq_len, 1)  # (batch_size, max_len, max_len)\n",
        "        return mask.to(self.device)\n",
        "\n",
        "    def _subsequent_mask(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"DecoderのMasked-Attentionに使用するmaskを作成する.\n",
        "        Parameters:\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            単語のトークン列. [batch_size, max_len, d_model]\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        max_len = x.size(1)\n",
        "        return (\n",
        "            torch.tril(torch.ones(batch_size, max_len, max_len)).eq(0).to(self.device)\n",
        "        )"
      ],
      "metadata": {
        "id": "ruU6m3yFyDYH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import join\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''\n",
        "from const.path import (\n",
        "    FIGURE_PATH,\n",
        "    KFTT_TOK_CORPUS_PATH,\n",
        "    NN_MODEL_PICKLES_PATH,\n",
        "    TANAKA_CORPUS_PATH,\n",
        ")\n",
        "'''\n",
        "#from models import Transformer\n",
        "#from utils.dataset.Dataset import KfttDataset\n",
        "#from utils.evaluation.bleu import BleuScore\n",
        "#from utils.text.text import tensor_to_text, text_to_tensor\n",
        "#from utils.text.vocab import get_vocab\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        net: nn.Module,\n",
        "        optimizer: optim.Optimizer,\n",
        "        critetion: nn.Module,\n",
        "        bleu_score: BleuScore,\n",
        "        device: torch.device,\n",
        "    ) -> None:\n",
        "        self.net = net\n",
        "        self.optimizer = optimizer\n",
        "        self.critetion = critetion\n",
        "        self.device = device\n",
        "        self.bleu_score = bleu_score\n",
        "        self.net = self.net.to(self.device)\n",
        "\n",
        "    def loss_fn(self, preds: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        return self.critetion(preds, labels)\n",
        "\n",
        "    def train_step(\n",
        "        self, src: torch.Tensor, tgt: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, float]:\n",
        "        self.net.train()\n",
        "        output = self.net(src, tgt)\n",
        "\n",
        "        tgt = tgt[:, 1:]  # decoderからの出力は1 ~ max_lenまでなので、0以降のデータで誤差関数を計算する\n",
        "        output = output[:, :-1, :]  #\n",
        "\n",
        "        # calculate loss\n",
        "        loss = self.loss_fn(\n",
        "            output.contiguous().view(\n",
        "                -1,\n",
        "                output.size(-1),\n",
        "            ),\n",
        "            tgt.contiguous().view(-1),\n",
        "        )\n",
        "\n",
        "        # calculate bleu score\n",
        "        _, output_ids = torch.max(output, dim=-1)\n",
        "        bleu_score = self.bleu_score(tgt, output_ids)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, output, bleu_score\n",
        "\n",
        "    def val_step(\n",
        "        self, src: torch.Tensor, tgt: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, float]:\n",
        "        self.net.eval()\n",
        "        output = self.net(src, tgt)\n",
        "\n",
        "        tgt = tgt[:, 1:]\n",
        "        output = output[:, :-1, :]  #\n",
        "\n",
        "        loss = self.loss_fn(\n",
        "            output.contiguous().view(\n",
        "                -1,\n",
        "                output.size(-1),\n",
        "            ),\n",
        "            tgt.contiguous().view(-1),\n",
        "        )\n",
        "        _, output_ids = torch.max(output, dim=-1)\n",
        "        bleu_score = self.bleu_score(tgt, output_ids)\n",
        "\n",
        "        return loss, output, bleu_score\n",
        "\n",
        "    def fit(\n",
        "        self, train_loader: DataLoader, val_loader: DataLoader, print_log: bool = True\n",
        "    ) -> Tuple[List[float], List[float], List[float], List[float]]:\n",
        "        # train\n",
        "        train_losses: List[float] = []\n",
        "        train_bleu_scores: List[float] = []\n",
        "        if print_log:\n",
        "            print(f\"{'-'*20 + 'Train' + '-'*20} \\n\")\n",
        "        for i, (src, tgt) in enumerate(train_loader):\n",
        "            src = src.to(self.device)\n",
        "            tgt = tgt.to(self.device)\n",
        "            loss, _, bleu_score = self.train_step(src, tgt)\n",
        "            src = src.to(\"cpu\")\n",
        "            tgt = tgt.to(\"cpu\")\n",
        "\n",
        "            if print_log:\n",
        "                print(\n",
        "                    f\"train loss: {loss.item()}, bleu score: {bleu_score},\"\n",
        "                    + f\"iter: {i+1}/{len(train_loader)} \\n\"\n",
        "                )\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            train_bleu_scores.append(bleu_score)\n",
        "\n",
        "        # validation\n",
        "        val_losses: List[float] = []\n",
        "        val_bleu_scores: List[float] = []\n",
        "        if print_log:\n",
        "            print(f\"{'-'*20 + 'Validation' + '-'*20} \\n\")\n",
        "        for i, (src, tgt) in enumerate(val_loader):\n",
        "            src = src.to(self.device)\n",
        "            tgt = tgt.to(self.device)\n",
        "            loss, _, bleu_score = self.val_step(src, tgt)\n",
        "            src = src.to(\"cpu\")\n",
        "            tgt = tgt.to(\"cpu\")\n",
        "\n",
        "            if print_log:\n",
        "                print(f\"train loss: {loss.item()}, iter: {i+1}/{len(val_loader)} \\n\")\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            val_bleu_scores.append(bleu_score)\n",
        "\n",
        "        return train_losses, train_bleu_scores, val_losses, val_bleu_scores\n",
        "\n",
        "    def test(self, test_data_loader: DataLoader) -> Tuple[List[float], List[float]]:\n",
        "        test_losses: List[float] = []\n",
        "        test_bleu_scores: List[float] = []\n",
        "        for i, (src, tgt) in enumerate(test_data_loader):\n",
        "            src = src.to(self.device)\n",
        "            tgt = tgt.to(self.device)\n",
        "            loss, _, bleu_score = trainer.val_step(src, tgt)\n",
        "            src = src.to(\"cpu\")\n",
        "            tgt = tgt.to(\"cpu\")\n",
        "\n",
        "            test_losses.append(loss.item())\n",
        "            test_bleu_scores.append(bleu_score)\n",
        "\n",
        "        return test_losses, test_bleu_scores\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"\n",
        "    1.define path & create vocab\n",
        "    \"\"\"\n",
        "    TRAIN_SRC_CORPUS_PATH = join(KFTT_TOK_CORPUS_PATH, \"kyoto-train.en\")\n",
        "    TRAIN_TGT_CORPUS_PATH = join(KFTT_TOK_CORPUS_PATH, \"kyoto-train.ja\")\n",
        "\n",
        "    VAL_SRC_CORPUS_PATH = join(KFTT_TOK_CORPUS_PATH, \"kyoto-dev.en\")\n",
        "    VAL_TGT_CORPUS_PATH = join(KFTT_TOK_CORPUS_PATH, \"kyoto-dev.ja\")\n",
        "\n",
        "    TEST_SRC_CORPUS_PATH = join(KFTT_TOK_CORPUS_PATH, \"kyoto-test.en\")\n",
        "    TEST_TGT_CORPUS_PATH = join(KFTT_TOK_CORPUS_PATH, \"kyoto-test.ja\")\n",
        "\n",
        "    src_vocab = get_vocab(TRAIN_SRC_CORPUS_PATH, vocab_size=20000)\n",
        "    tgt_vocab = get_vocab(TRAIN_TGT_CORPUS_PATH, vocab_size=20000)\n",
        "\n",
        "    \"\"\"\n",
        "    2.Define Parameters # TODO: from arguement or config file(hydra)\n",
        "    \"\"\"\n",
        "    src_vocab_size = len(src_vocab)\n",
        "    tgt_vocab_size = len(tgt_vocab)\n",
        "    max_len = 24\n",
        "    d_model = 128\n",
        "    heads_num = 4\n",
        "    d_ff = 256\n",
        "    N = 3\n",
        "    dropout_rate = 0.1\n",
        "    layer_norm_eps = 1e-8\n",
        "    pad_idx = 0\n",
        "    batch_size = 100\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    epoch = 10\n",
        "\n",
        "    \"\"\"\n",
        "    3.Define model\n",
        "    \"\"\"\n",
        "    net = Transformer(\n",
        "        src_vocab_size=src_vocab_size,\n",
        "        tgt_vocab_size=tgt_vocab_size,\n",
        "        max_len=max_len,\n",
        "        d_model=d_model,\n",
        "        heads_num=heads_num,\n",
        "        d_ff=d_ff,\n",
        "        N=N,\n",
        "        dropout_rate=dropout_rate,\n",
        "        layer_norm_eps=layer_norm_eps,\n",
        "        pad_idx=pad_idx,\n",
        "        device=device,\n",
        "    )\n",
        "    \"\"\"\n",
        "    4.Define dataset & dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    def src_text_to_tensor(text: str, max_len: int) -> torch.Tensor:\n",
        "        return text_to_tensor(text, src_vocab, max_len, eos=False, bos=False)\n",
        "\n",
        "    def src_tensor_to_text(tensor: torch.Tensor) -> str:\n",
        "        return tensor_to_text(tensor, src_vocab)\n",
        "\n",
        "    def tgt_text_to_tensor(text: str, max_len: int) -> torch.Tensor:\n",
        "        return text_to_tensor(text, tgt_vocab, max_len)\n",
        "\n",
        "    def tgt_tensor_to_text(tensor: torch.Tensor) -> str:\n",
        "        return tensor_to_text(tensor, tgt_vocab)\n",
        "\n",
        "    train_dataset = KfttDataset(\n",
        "        TRAIN_SRC_CORPUS_PATH,\n",
        "        TRAIN_TGT_CORPUS_PATH,\n",
        "        max_len,\n",
        "        src_text_to_tensor,\n",
        "        tgt_text_to_tensor,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    val_dataset = KfttDataset(\n",
        "        VAL_SRC_CORPUS_PATH,\n",
        "        VAL_TGT_CORPUS_PATH,\n",
        "        max_len,\n",
        "        src_text_to_tensor,\n",
        "        tgt_text_to_tensor,\n",
        "    )\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    TEST_SRC_CORPUS_PATH\n",
        "    TEST_TGT_CORPUS_PATH\n",
        "    test_dataset = KfttDataset(\n",
        "        TEST_SRC_CORPUS_PATH,\n",
        "        TEST_TGT_CORPUS_PATH,\n",
        "        max_len,\n",
        "        src_text_to_tensor,\n",
        "        tgt_text_to_tensor,\n",
        "    )\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    \"\"\"\n",
        "    5.Train\n",
        "    \"\"\"\n",
        "    trainer = Trainer(\n",
        "        net,\n",
        "        optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), amsgrad=True),\n",
        "        nn.CrossEntropyLoss(),\n",
        "        BleuScore(tgt_vocab),\n",
        "        device,\n",
        "    )\n",
        "    train_losses: List[float] = []\n",
        "    train_bleu_scores: List[float] = []\n",
        "    val_losses: List[float] = []\n",
        "\n",
        "    for i in range(epoch):\n",
        "        print(f\"epoch: {i + 1} \\n\")\n",
        "        (\n",
        "            train_losses_per_epoch,\n",
        "            train_bleu_scores_per_epoch,\n",
        "            val_losses_per_epoch,\n",
        "            val_bleu_scores_per_epoch,\n",
        "        ) = trainer.fit(train_loader, val_loader, print_log=True)\n",
        "\n",
        "        train_losses.extend(train_losses_per_epoch)\n",
        "        train_bleu_scores.extend(train_bleu_scores_per_epoch)\n",
        "        val_losses.extend(val_losses_per_epoch)\n",
        "        torch.save(trainer.net, join(NN_MODEL_PICKLES_PATH, f\"epoch_{i}.pt\"))\n",
        "\n",
        "    \"\"\"\n",
        "    6.Test\n",
        "    \"\"\"\n",
        "    test_losses, test_bleu_scores = trainer.test(test_loader)\n",
        "    \"\"\"\n",
        "    7.Plot & save\n",
        "    \"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(24, 8))\n",
        "    train_loss_ax = fig.add_subplot(1, 2, 1)\n",
        "    val_loss_ax = fig.add_subplot(1, 2, 2)\n",
        "\n",
        "    train_loss_ax.plot(list(range(len(train_losses))), train_losses, label=\"train loss\")\n",
        "    val_loss_ax.plot(\n",
        "        list(range(len(train_bleu_scores))),\n",
        "        train_bleu_scores,\n",
        "        label=\"val loss\",\n",
        "    )\n",
        "    train_loss_ax.legend()\n",
        "    val_loss_ax.legend()\n",
        "    plt.savefig(join(FIGURE_PATH, \"loss.png\"))"
      ],
      "metadata": {
        "id": "tnuoRnwGzYSe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "83ac2dc2-37af-4762-cef8-fd933cdb1786"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9f0f6ff2cd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     def __init__(\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9f0f6ff2cd6d>\u001b[0m in \u001b[0;36mTrainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcritetion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mbleu_score\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBleuScore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     ) -> None:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BleuScore' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wToUNkDR02io"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}