{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d1e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "seq2seqよりはやい\n",
    "RNN、CNNの使用をしていない\n",
    "Attentionで並列処理\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#流れ\n",
    "\n",
    "'''\n",
    "①input (embedding)\n",
    "②self attention\n",
    "③\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "import numpy as np\n",
    "#from layers.transformer.ScaledDotProductAttention import ScaledDotProductAttention\n",
    "\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "#from .Embedding import Embedding\n",
    "#from .FFN import FFN\n",
    "#from .MultiHeadAttention import MultiHeadAttention\n",
    "#from .PositionalEncoding import AddPositionalEncoding\n",
    "\n",
    "    \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "#from .Embedding import Embedding\n",
    "#from .FFN import FFN\n",
    "#from .MultiHeadAttention import MultiHeadAttention\n",
    "#from .PositionalEncoding import AddPositionalEncoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, pad_idx: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=pad_idx\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding_layer(input_batch)\n",
    "    \n",
    "    \n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(relu(self.linear1(x)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class AddPositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int, max_len: int, device: torch.device = torch.device(\"cpu\")\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        positional_encoding_weight: torch.Tensor = self._initialize_weight().to(device)\n",
    "        self.register_buffer(\"positional_encoding_weight\", positional_encoding_weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.positional_encoding_weight[:seq_len, :].unsqueeze(0)\n",
    "\n",
    "    def _get_positional_encoding(self, pos: int, i: int) -> float:\n",
    "        w = pos / (10000 ** (((2 * i) // 2) / self.d_model))\n",
    "        if i % 2 == 0:\n",
    "            return np.sin(w)\n",
    "        else:\n",
    "            return np.cos(w)\n",
    "\n",
    "    def _initialize_weight(self) -> torch.Tensor:\n",
    "        positional_encoding_weight = [\n",
    "            [self._get_positional_encoding(pos, i) for i in range(1, self.d_model + 1)]\n",
    "            for pos in range(1, self.max_len + 1)\n",
    "        ]\n",
    "        return torch.tensor(positional_encoding_weight).float()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.d_v = d_model // h\n",
    "\n",
    "        #\n",
    "        self.W_k = nn.Parameter(\n",
    "            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)\n",
    "        )\n",
    "\n",
    "        self.W_q = nn.Parameter(\n",
    "            torch.Tensor(h, d_model, self.d_k)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)\n",
    "        )\n",
    "\n",
    "        self.W_v = nn.Parameter(\n",
    "            torch.Tensor(h, d_model, self.d_v)  # ヘッド数, 入力次元, 出力次元(=入力次元/ヘッド数)\n",
    "        )\n",
    "\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "        self.linear = nn.Linear(h * self.d_v, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask_3d: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        batch_size, seq_len = q.size(0), q.size(1)\n",
    "\n",
    "        \"\"\"repeat Query,Key,Value by num of heads\"\"\"\n",
    "        q = q.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model\n",
    "        k = k.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model\n",
    "        v = v.repeat(self.h, 1, 1, 1)  # head, batch_size, seq_len, d_model\n",
    "\n",
    "        \"\"\"Linear before scaled dot product attention\"\"\"\n",
    "        q = torch.einsum(\n",
    "            \"hijk,hkl->hijl\", (q, self.W_q)\n",
    "        )  # head, batch_size, d_k, seq_len\n",
    "        k = torch.einsum(\n",
    "            \"hijk,hkl->hijl\", (k, self.W_k)\n",
    "        )  # head, batch_size, d_k, seq_len\n",
    "        v = torch.einsum(\n",
    "            \"hijk,hkl->hijl\", (v, self.W_v)\n",
    "        )  # head, batch_size, d_k, seq_len\n",
    "\n",
    "        \"\"\"Split heads\"\"\"\n",
    "        q = q.view(self.h * batch_size, seq_len, self.d_k)\n",
    "        k = k.view(self.h * batch_size, seq_len, self.d_k)\n",
    "        v = v.view(self.h * batch_size, seq_len, self.d_v)\n",
    "\n",
    "        if mask_3d is not None:\n",
    "            mask_3d = mask_3d.repeat(self.h, 1, 1)\n",
    "\n",
    "        \"\"\"Scaled dot product attention\"\"\"\n",
    "        attention_output = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask_3d\n",
    "        )  # (head*batch_size, seq_len, d_model)\n",
    "\n",
    "        attention_output = torch.chunk(attention_output, self.h, dim=0)\n",
    "        attention_output = torch.cat(attention_output, dim=2)\n",
    "\n",
    "        \"\"\"Linear after scaled dot product attention\"\"\"\n",
    "        output = self.linear(attention_output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,  # target\n",
    "        k: torch.Tensor,  # source\n",
    "        v: torch.Tensor,  # source\n",
    "        mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        scalar = np.sqrt(self.d_k)\n",
    "        attention_weight = torch.matmul(q, torch.transpose(k, 1, 2)) / scalar\n",
    "        if mask is not None:\n",
    "            if mask.dim() != attention_weight.dim():\n",
    "                raise ValueError(\n",
    "                    \"mask.dim != attention_weight.dim, mask.dim={}, attention_weight.dim={}\".format(\n",
    "                        mask.dim(), attention_weight.dim()\n",
    "                    )\n",
    "                )\n",
    "            attention_weight = attention_weight.data.masked_fill_(\n",
    "                mask, -torch.finfo(torch.float).max\n",
    "            )\n",
    "        attention_weight = nn.functional.softmax(attention_weight, dim=2)\n",
    "        return torch.matmul(attention_weight, v)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        heads_num: int,\n",
    "        dropout_rate: float,\n",
    "        layer_norm_eps: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, heads_num)\n",
    "        self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm_self_attention = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.src_tgt_attention = MultiHeadAttention(d_model, heads_num)\n",
    "        self.dropout_src_tgt_attention = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm_src_tgt_attention = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm_ffn = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,  # Decoder input\n",
    "        src: torch.Tensor,  # Encoder output\n",
    "        mask_src_tgt: torch.Tensor,\n",
    "        mask_self: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        tgt = self.layer_norm_self_attention(\n",
    "            tgt + self.__self_attention_block(tgt, mask_self)\n",
    "        )\n",
    "\n",
    "        x = self.layer_norm_src_tgt_attention(\n",
    "            tgt + self.__src_tgt_attention_block(src, tgt, mask_src_tgt)\n",
    "        )\n",
    "\n",
    "        x = self.layer_norm_ffn(x + self.__feed_forward_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __src_tgt_attention_block(\n",
    "        self, src: torch.Tensor, tgt: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return self.dropout_src_tgt_attention(\n",
    "            self.src_tgt_attention(tgt, src, src, mask)\n",
    "        )\n",
    "\n",
    "    def __self_attention_block(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        return self.dropout_self_attention(self.self_attention(x, x, x, mask))\n",
    "\n",
    "    def __feed_forward_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout_ffn(self.ffn(x))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tgt_vocab_size: int,\n",
    "        max_len: int,\n",
    "        pad_idx: int,\n",
    "        d_model: int,\n",
    "        N: int,\n",
    "        d_ff: int,\n",
    "        heads_num: int,\n",
    "        dropout_rate: float,\n",
    "        layer_norm_eps: float,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(tgt_vocab_size, d_model, pad_idx)\n",
    "        self.positional_encoding = AddPositionalEncoding(d_model, max_len, device)\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(\n",
    "                    d_model, d_ff, heads_num, dropout_rate, layer_norm_eps\n",
    "                )\n",
    "                for _ in range(N)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,  # Decoder input\n",
    "        src: torch.Tensor,  # Encoder output\n",
    "        mask_src_tgt: torch.Tensor,\n",
    "        mask_self: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            tgt = decoder_layer(\n",
    "                tgt,\n",
    "                src,\n",
    "                mask_src_tgt,\n",
    "                mask_self,\n",
    "            )\n",
    "        return tgt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        heads_num: int,\n",
    "        dropout_rate: float,\n",
    "        layer_norm_eps: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, heads_num)\n",
    "        self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm_self_attention = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm_ffn = LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = self.layer_norm_self_attention(self.__self_attention_block(x, mask) + x)\n",
    "        x = self.layer_norm_ffn(self.__feed_forward_block(x) + x)\n",
    "        return x\n",
    "\n",
    "    def __self_attention_block(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        self attention block\n",
    "        \"\"\"\n",
    "        x = self.multi_head_attention(x, x, x, mask)\n",
    "        return self.dropout_self_attention(x)\n",
    "\n",
    "    def __feed_forward_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        feed forward block\n",
    "        \"\"\"\n",
    "        return self.dropout_ffn(self.ffn(x))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_len: int,\n",
    "        pad_idx: int,\n",
    "        d_model: int,\n",
    "        N: int,\n",
    "        d_ff: int,\n",
    "        heads_num: int,\n",
    "        dropout_rate: float,\n",
    "        layer_norm_eps: float,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, pad_idx)\n",
    "\n",
    "        self.positional_encoding = AddPositionalEncoding(d_model, max_len, device)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(\n",
    "                    d_model, d_ff, heads_num, dropout_rate, layer_norm_eps\n",
    "                )\n",
    "                for _ in range(N)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28af6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
